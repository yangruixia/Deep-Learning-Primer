# 1. Pytorch介绍

Pytorch是一个开源的深度学习框架，主要用于构建、训练和部署神经网络模型。它由Facebook的人工智能研究团队开发，并于2016年首次发布。类似于numpy但是可以进行GPU加速。

除了Pytorch，其他流行的深度学习框架还有：
- TensorFlow: TensorFlow是一个由Google开发的开源深度学习框架，具有广泛的应用和强大的生态系统。它使用静态图计算，支持高性能的分布式训练和推理，并提供了丰富的工具和库。
- Keras: Keras是一个高级深度学习框架，可以作为TensorFlow、Theano或Microsoft Cognitive Toolkit的前端接口。它简化了模型的构建和训练过程，并提供了丰富的预训练模型和工具。

相较于其他深度学习框架，Pytorch的优势在于：
1. 动态图计算：Pytorch采用了动态图计算的方式，相比于静态图计算的框架（如TensorFlow），更加灵活和直观。
2. 自动求导：Pytorch内置了自动求导功能，能够自动计算张量的梯度。
3. Pytorch在学术界和工业界都有广泛的应用，许多顶级的研究机构和公司都在使用Pytorch进行深度学习研究和应用开发。

# 2. Pytorch入门

## 2.1 Pytorch入门推荐教程

- 官方文档：[Pytorch官网](https://pytorch.org/docs/stable/index.html)

- 李沐教程：
  - [动手学深度学习文档](https://zh.d2l.ai/)
  - [动手学深度学习配套视频教程](https://www.bilibili.com/video/BV1if4y147hS/?share_source=copy_web&vd_source=c6cc3d47bc52080e2c91b22facc5dbcd)

- B站最详细的Pytorch教程：[小土堆视频教程](https://www.bilibili.com/video/BV1hE411t7RN/?share_source=copy_web&vd_source=c6cc3d47bc52080e2c91b22facc5dbcd)，偏向图像方面，选择性观看即可。

- [NLP模型讲解推荐](https://space.bilibili.com/383551518/channel/collectiondetail?sid=463688)

## 2.2 入门索引

### 矩阵初始化
初始化空矩阵
```
x = torch.empty(5, 3)
```
初始化随机矩阵
```
x = torch.rand(5, 3)
```
初始化零矩阵
```
x = torch.zeros(5,3,dtype=torch.long)
```
初始化和别的tensor形状相同的一矩阵
```
y = torch.ones_like(x)
```

### 矩阵转化
Python数组转化为Tensor
```
x = torch.tensor([5,3])
```
Tensor转化为numpy数组
```
import torch
a = torch.ones(5)
# 将a转化为numpy赋值给b
b = a.numpy()
```
numpy转化为Tensor
```
import torch
import numpy as np
a = np.ones(5)
b = torch.from_numpy(a)
```
### 查看Tensor
查看Tensor的形状
```
print(x.size())
# torch.size其实是一个tuple，因此它支持所有的tuple操作。
```
查看Tensor的某行/某列
```
# 打印x的第一列
print(x[:, 1])
# 打印x的第一行
print(x[1, :])
```

### Tensor与GPU

检查cuda环境是否可用
```
if torch.cuda.is_available():
    device = torch.device("cuda")
```
在GPU上创建Tensor
```
if torch.cuda.is_available():
    device = torch.device("cuda")
    x = torch.ones(3,5,device = device)
```

使用to方法转移Tensor到任何设备上
```
x = torch.zeros(3,5)
if torch.cuda.is_available():
    device = torch.device("cuda")
    x = x.to(device)
```


# 3. Pytorch进阶

## 3.1 训练一个神经网络通常需要如下步骤
### 引入模块
```
import torch
import torch.nn as nn
import torch.nn.functional as F
```
### 定义模型
我们只需定义模型的forward函数，而backward函数会自动通过autograd创建。

### 处理网络的输入
torch.nn只支持mini-batches的输入，整个torch.nn包的输入都必须第一维是batch，即使只有一个样本也要弄成batch是1的输入。

比如nn.Conv2d的输入是一个4D的Tensor，shape是nSamples nChannels Height Width，如果你只有一个样本 nChannels Height Width，那么可以使用input.unsqueeze（0）来增加一个batch维，相反地，squeeze用于降维。


### 计算loss
损失函数的参数是（output，target）对，output是模型的预测，target是实际的值，损失函数会计算预测值和真实值的差别，损失越小说明预测的越准。

### 计算loss对参数的梯度
在调用loss.backward（）之前，需要清除tensor里之前的梯度，否则会累加进去。
```
net.zero_grad()
```
### 更新参数
更新参数最简单的方法是使用随机梯度下降SGD
```
Weight = weight - learning rate*gradient
weight=weight-learning_rate*gradient
```
### GPU上训练
为了在GPU上训练，我们需要把Tensor转移到GPU上。

首先我们看看是否有GPU，如果没有，我们还是fallback到CPU上。


## 3.2 加载数据
### Dataset
提供一种方式获取数据及其对应的label值

### Dataloader
对于数据进行打包压缩，为后面的网络提供不同的数据形式。

### Transformers

## 3.3 加载TensorBoard（可选）

## 3.4 nn.Module
nn是neural network

### 线性层
Linear Layer 实现对前一层的线性变换。


### 卷积层
- in_channels输入图片的channel数
- out_channels输出图片的channel数
- kernel_size训练过程中对于kernel进行调整
- Stride=1
- Padding=0
- padding_mode
- dilation

### 池化层

### 非线性激活
- Relu
- Sigmoid

### Transformer



